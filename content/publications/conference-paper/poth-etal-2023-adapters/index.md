---
title: "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning"
authors:
  - Clifton Poth
  - Hannah Sterz
  - Indraneil Paul
  - Sukannya Purkayastha
  - Leon Engländer
  - Timo Imhof
  - Ivan Vulić
  - Sebastian Ruder
  - Iryna Gurevych
  - Jonas Pfeiffer
date: "2023-12-01T00:00:00Z"
publishDate: "2023-12-01T00:00:00Z"
publication_types: ["inproceedings"]
publication: "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
publication_short: "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
abstract: |
  We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.
summary: "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning — system demo at EMNLP 2023."
tags:
  - Transfer Learning
  - Adapters
  - NLP
featured: true
links:
  - type: proceedings
    url: "https://aclanthology.org/2023.emnlp-demo.13/"
  - type: code
    url: "https://adapterhub.ml/adapters"
---

This page contains the bibliographic metadata and links for the Adapters system demo (EMNLP 2023). Add notes, supplementary material, or the full text below.

## BibTeX

```bibtex
@inproceedings{poth-etal-2023-adapters,
    title = "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning",
    author = {Poth, Clifton  and
      Sterz, Hannah  and
      Paul, Indraneil  and
      Purkayastha, Sukannya  and
      Engl{"a}nder, Leon  and
      Imhof, Timo  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Gurevych, Iryna  and
      Pfeiffer, Jonas},
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-demo.13/",
  % doi removed
    pages = "149--160",
    abstract = "We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library{'}s efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters." 
}
```
